{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "app_root = '../../../../'\n",
    "wiki_downloaded_dir_path = '/mnt/c/Users/Marco/Downloads'\n",
    "wiki_save_path = os.path.join(app_root, 'model', 'common', 'wiki_dumps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, wget\n",
    "\n",
    "def download_wiki(lang = 'en', dir_path = './'):\n",
    "    ''' Download latest Wikipedia dump in chosen language '''\n",
    "\n",
    "    def bar_progress(current, total, width=80):\n",
    "        current = current * 1e-6\n",
    "        total = total * 1e-6\n",
    "        progress_message = \"Downloading: %d%% [%d / %d] MB\" % (current / total * 100, current, total)\n",
    "        sys.stdout.write(\"\\r\" + progress_message)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    lang = lang.lower()\n",
    "    WIKI_DUMP_NAME = f'{lang}wiki-latest-pages-articles.xml.bz2'\n",
    "    WIKI_DUMP_URL = f'https://dumps.wikimedia.org/{lang}wiki/latest/{WIKI_DUMP_NAME}'\n",
    "\n",
    "    save_path_name = os.path.join(dir_path, WIKI_DUMP_NAME)\n",
    "\n",
    "    if not os.path.exists(save_path_name):\n",
    "        print(f'Downloading the latest {lang}-language Wikipedia dump from {WIKI_DUMP_URL}...')\n",
    "        save_path_name = wget.download( WIKI_DUMP_URL, save_path_name, bar=bar_progress )\n",
    "        print(f'Successfully downloaded!')\n",
    "    else:\n",
    "        print(f'Already downloaded!')\n",
    "\n",
    "    return save_path_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wiki(path_to_file, path_dir_out = None):\n",
    "    file_name_ext_in = os.path.basename(path_to_file)\n",
    "    file_name_ext_out = file_name_ext_in.split('.')[0] + '.txt'\n",
    "    path_file_out = os.path.join( os.path.dirname(path_to_file) if path_dir_out is None else path_dir_out , file_name_ext_out )\n",
    "    if not os.path.exists(path_file_out):\n",
    "        print(f'Extracting and cleaning {path_to_file} to {path_file_out}...')\n",
    "        ! python3 -m wikiextractor.WikiExtractor  $path_to_file --processes 79 -q -o - \\\n",
    "        | sed \"/^\\s*\\$/d\" \\\n",
    "        | grep -v \"^<doc id=\" \\\n",
    "        | grep -v \"</doc>\\$\" \\\n",
    "        > $path_file_out\n",
    "        print(f'Successfully extracted!')\n",
    "    else:\n",
    "        print(f'Already extracted!')\n",
    "    return path_file_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from blingfire import text_to_sentences\n",
    "from random import random\n",
    "\n",
    "def preprocess_wiki(path_to_file, path_dir_out = None, max_n_sentences = 100000):\n",
    "    MIN_SENTENCE_LEN = 16\n",
    "    MAX_SENTENCE_LEN = 110\n",
    "    PERCENTAGE_SENTENCE = 0.01\n",
    "    file_name_ext_in = os.path.basename(path_to_file)\n",
    "    file_name_ext_out = file_name_ext_in.split('.')[0] + '_preprocessed.txt'\n",
    "    path_file_out = os.path.join( os.path.dirname(path_to_file) if path_dir_out is None else path_dir_out , file_name_ext_out )\n",
    "    print(f'Preprocessing {path_to_file} to {path_file_out}...')\n",
    "    counter = 0\n",
    "    with open(path_file_out, 'w', encoding='utf-8') as out_f:\n",
    "        with open(path_to_file, 'r', encoding='utf-8') as in_f:\n",
    "            for line in in_f:\n",
    "                if counter >= max_n_sentences:\n",
    "                    break\n",
    "                sentences = text_to_sentences(line)\n",
    "                for sentence in sentences.split('\\n'):\n",
    "                    if len(sentence.split()) >= MIN_SENTENCE_LEN and len(sentence.split()) <= MAX_SENTENCE_LEN and (random() <= PERCENTAGE_SENTENCE):\n",
    "                        counter += 1\n",
    "                        out_f.write(sentence + ('\\n' if counter < max_n_sentences else '') )\n",
    "                        \n",
    "    print(f'Successfully preprocessed {counter} lines!')\n",
    "    return path_file_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already downloaded!\n"
     ]
    }
   ],
   "source": [
    "saved_path_en = download_wiki(lang = 'en', dir_path = wiki_downloaded_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already extracted!\n"
     ]
    }
   ],
   "source": [
    "saved_path_extracted_en = extract_wiki(saved_path_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing /mnt/c/Users/Marco/Downloads/enwiki-latest-pages-articles.txt to ../../../../model/common/wiki_dumps/enwiki-latest-pages-articles_preprocessed.txt...\n",
      "Successfully preprocessed 100000 lines!\n"
     ]
    }
   ],
   "source": [
    "saved_path_preprocessed_en = preprocess_wiki(saved_path_extracted_en, path_dir_out = wiki_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spanish Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already downloaded!\n"
     ]
    }
   ],
   "source": [
    "saved_path_es = download_wiki(lang = 'es', dir_path = wiki_downloaded_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already extracted!\n"
     ]
    }
   ],
   "source": [
    "saved_path_extracted_es = extract_wiki(saved_path_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing /mnt/c/Users/Marco/Downloads/eswiki-latest-pages-articles.txt to ../../../../model/common/wiki_dumps/eswiki-latest-pages-articles_preprocessed.txt...\n",
      "Successfully preprocessed 100000 lines!\n"
     ]
    }
   ],
   "source": [
    "saved_path_preprocessed_es = preprocess_wiki(saved_path_extracted_es, path_dir_out = wiki_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# French Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already downloaded!\n"
     ]
    }
   ],
   "source": [
    "saved_path_fr = download_wiki(lang = 'fr', dir_path = wiki_downloaded_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already extracted!\n"
     ]
    }
   ],
   "source": [
    "saved_path_extracted_fr = extract_wiki(saved_path_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing /mnt/c/Users/Marco/Downloads/frwiki-latest-pages-articles.txt to ../../../../model/common/wiki_dumps/frwiki-latest-pages-articles_preprocessed.txt...\n",
      "Successfully preprocessed 100000 lines!\n"
     ]
    }
   ],
   "source": [
    "saved_path_preprocessed_fr = preprocess_wiki(saved_path_extracted_fr, path_dir_out = wiki_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbatlas_dependency_url = 'http://127.0.0.1:3003/api/model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate dataset (Spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def generate_data(dump_path, verbatlas_link, lang=\"EN\", chunk_dim=1, n_max_sentences = 8_000):\n",
    "    null_tag = '_'\n",
    "    result = {}\n",
    "\n",
    "    sentences_skipped = 0\n",
    "    sentences_total = 0\n",
    "    to_send = []\n",
    "\n",
    "    with open(dump_path, 'r') as file:\n",
    "        pbar = tqdm(enumerate(file))\n",
    "        for data_i, line in pbar:\n",
    "            data_text = line.rstrip()\n",
    "\n",
    "            to_send.append({ 'text': data_text, 'lang':lang, 'key_id':lang+'_n_'+str(data_i) })\n",
    "\n",
    "            sentences_total += 1\n",
    "            if sentences_total - sentences_skipped >= n_max_sentences:\n",
    "                break\n",
    "\n",
    "            if len(to_send)%chunk_dim != 0:\n",
    "                continue\n",
    "            \n",
    "            pbar.set_description(f'index={data_i}')\n",
    "\n",
    "            res_v = requests.post(verbatlas_link, json = to_send)\n",
    "            status_code_good = res_v.status_code == 200\n",
    "\n",
    "            if not status_code_good:\n",
    "                print(f'verbatlas={res_v.status_code} | skipping chunk!')\n",
    "                sentences_skipped += len(to_send)\n",
    "                to_send = []\n",
    "\n",
    "            res_v = json.loads(res_v.text)\n",
    "            for t_i, r_v_i in zip(to_send, res_v):\n",
    "\n",
    "                k_id = t_i['key_id']\n",
    "\n",
    "                result[k_id] = {'roles':{}}\n",
    "                # words\n",
    "                result[k_id]['words'] = [ e['rawText'] for e in r_v_i['tokens'] ]\n",
    "\n",
    "                # predicates + roles\n",
    "                result[k_id]['predicates'] = [null_tag]*len(r_v_i['tokens'])\n",
    "                for annotation in r_v_i['annotations']:\n",
    "                    tokenIndex = annotation['tokenIndex']\n",
    "                    if annotation['verbatlas']['frameName'] != null_tag:\n",
    "                        result[k_id]['predicates'][tokenIndex] = annotation['verbatlas']['frameName']\n",
    "\n",
    "                        result[k_id]['roles'][str(tokenIndex)] = [null_tag]*len(r_v_i['tokens'])\n",
    "\n",
    "                        for _, role_for_i in enumerate(annotation['verbatlas']['roles']):\n",
    "                            span_pos = role_for_i['span'][0]\n",
    "                                    \n",
    "                            result[k_id]['roles'][str(tokenIndex)][span_pos] = role_for_i['role'].lower()\n",
    "                    \n",
    "            to_send = []\n",
    "    \n",
    "    print(f'skipped {sentences_skipped} sentences out of {sentences_total}')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "index=1883: : 1881it [01:40, 18.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verbatlas=500 | skipping chunk!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "index=3778: : 3777it [03:11, 21.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verbatlas=500 | skipping chunk!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "index=5361: : 5359it [04:24, 22.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verbatlas=500 | skipping chunk!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "index=5599: : 5598it [04:35, 21.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verbatlas=500 | skipping chunk!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "index=6448: : 6446it [05:15, 21.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verbatlas=500 | skipping chunk!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "index=8003: : 8004it [06:29, 20.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped 5 sentences out of 8005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "es_wiki_gen = generate_data(os.path.join(wiki_save_path,'eswiki-latest-pages-articles_preprocessed.txt'), verbatlas_dependency_url, lang=\"ES\", chunk_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join(app_root,'data','ES','train_wiki_es.json'), 'w') as outfile:\n",
    "    json.dump(es_wiki_gen, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "176726de92aa5f7c7bad0ace42430d0e9e67427a9b4905ef1c48cfd9a71ff23e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nlp2022-hw2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
